{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Loading and Preparation\n",
    "# Study Dataset: C4 Questionnaire Data\n",
    "\n",
    "# This notebook provides the initial setup for loading and preparing the dataset for:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Statistical Analysis  \n",
    "- Machine Learning Modeling\n",
    "\n",
    "# Dataset Overview:\n",
    "# The dataset contains questionnaire responses with various psychological scales:\n",
    "- SPQ: Schizotypal Personality Questionnaire\n",
    "- EQ: Empathy Quotient\n",
    "- SQR: Social Responsiveness Scale\n",
    "- AQ: Autism Spectrum Quotient\n",
    "- Diagnostic information: Various mental health diagnoses\n",
    "- Demographics: Age, sex, education, occupation, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Dataset\n",
    "DATA_PATH = '/Users/eb2007/Library/CloudStorage/OneDrive-UniversityofCambridge/Documents/PhD/data/data_c4_raw.csv'\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initial Data Exploration\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n=== MEMORY USAGE ===\")\n",
    "memory_usage = df.memory_usage(deep=True)\n",
    "print(f\"Total memory: {memory_usage.sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Average per column: {memory_usage.mean() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Check for missing values\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percent': missing_percent\n",
    "})\n",
    "\n",
    "missing_columns = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "if len(missing_columns) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    display(missing_columns)\n",
    "else:\n",
    "    print(\"No missing values detected!\")\n",
    "\n",
    "print(\"\\n=== SPECIAL MISSING INDICATORS ===\")\n",
    "special_missing = ['#NULL!', 'NULL', 'null', 'NaN', 'nan', 'NA', 'na', '']\n",
    "for indicator in special_missing:\n",
    "    count = (df == indicator).sum().sum()\n",
    "    if count > 0:\n",
    "        print(f\"'{indicator}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Data Cleaning and Preprocessing\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=== DATA CLEANING STEPS ===\")\n",
    "\n",
    "# Step 1: Replace special missing indicators with NaN\n",
    "special_missing_indicators = ['#NULL!', 'NULL', 'null', 'NaN', 'nan', 'NA', 'na', '']\n",
    "for indicator in special_missing_indicators:\n",
    "    df_clean = df_clean.replace(indicator, np.nan)\n",
    "\n",
    "print(f\"Step 1: Replaced special missing indicators with NaN\")\n",
    "\n",
    "# Step 2: Check actual column names\n",
    "print(\"\\nStep 2: Checking actual column names...\")\n",
    "\n",
    "# Check what diagnosis columns actually exist\n",
    "diagnosis_cols_actual = [col for col in df_clean.columns if 'diagnosis' in col.lower()]\n",
    "print(\"Actual diagnosis columns:\")\n",
    "for col in diagnosis_cols_actual:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Check what region/country columns exist\n",
    "region_cols = [col for col in df_clean.columns if 'region' in col.lower() or 'country' in col.lower()]\n",
    "print(\"\\nActual region/country columns:\")\n",
    "for col in region_cols:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Check questionnaire columns\n",
    "questionnaire_cols = [col for col in df_clean.columns if any(scale in col.lower() for scale in ['spq', 'eq', 'sqr', 'aq'])]\n",
    "print(\"\\nActual questionnaire columns:\")\n",
    "for col in questionnaire_cols:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Updated Column Definitions (with correct names)\n",
    "print(\"\\nStep 3: Categorizing columns...\")\n",
    "\n",
    "# Demographic columns\n",
    "demographic_cols = ['userid', 'age', 'sex', 'sex_filter', 'handedness', 'education', \n",
    "                   'occupation', 'countryregion', 'repeat']\n",
    "\n",
    "# Diagnosis columns (using actual names)\n",
    "diagnosis_cols = [col for col in df_clean.columns if col.startswith('diagnosis_')]\n",
    "autism_diagnosis_cols = [col for col in df_clean.columns if col.startswith('autism_diagnosis_')]\n",
    "\n",
    "# Questionnaire scale columns - CORRECTED NAMES\n",
    "spq_cols = [f'spq_{i}' for i in range(1, 11)]\n",
    "eq_cols = [f'eq_{i}' for i in range(1, 11)]\n",
    "sqr_cols = [f'sqr_{i}' for i in range(1, 11)]\n",
    "aq_cols = [f'aq_{i}' for i in range(1, 11)]\n",
    "\n",
    "# Total score columns\n",
    "total_cols = ['eq_10_total', 'sqr_10_total', 'spq_10_total', 'aq_10_total']\n",
    "\n",
    "# Z-score columns\n",
    "zscore_cols = ['zeq_10_total', 'zsqr_10_total', 'zspq_10_total', 'zaq_10_total']\n",
    "\n",
    "# T-score and other derived columns\n",
    "derived_cols = ['eq_tscore', 'sq_tscore', 'dscore_fromt', 'cscore_fromt', \n",
    "                'cognitivebraintype', 'sex_dichotomous', 'age_grouping', \n",
    "                'userid_grouping', 'STEMvsNOSTEM', 'autismvscontrols_1450matched']\n",
    "\n",
    "print(f\"Demographic columns: {len(demographic_cols)}\")\n",
    "print(f\"Diagnosis columns: {len(diagnosis_cols)}\")\n",
    "print(f\"Autism diagnosis columns: {len(autism_diagnosis_cols)}\")\n",
    "print(f\"SPQ columns: {len(spq_cols)}\")\n",
    "print(f\"EQ columns: {len(eq_cols)}\")\n",
    "print(f\"SQR columns: {len(sqr_cols)}\")\n",
    "print(f\"AQ columns: {len(aq_cols)}\")\n",
    "print(f\"Total score columns: {len(total_cols)}\")\n",
    "print(f\"Z-score columns: {len(zscore_cols)}\")\n",
    "print(f\"Derived columns: {len(derived_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Column Mappings (for columns that actually exist)\n",
    "column_mappings = {\n",
    "    'sex': {\n",
    "        1: 'Male',\n",
    "        2: 'Female',\n",
    "        3: 'Transgender or other',\n",
    "        4: 'Prefer not to say'\n",
    "    },\n",
    "    'handedness': {\n",
    "        1: 'Right-handed',\n",
    "        2: 'Left-handed',\n",
    "        3: 'Ambidextrous',\n",
    "        4: 'Prefer not to say'\n",
    "    },\n",
    "    'education': {\n",
    "        1: 'Did not complete High School (or A-levels)',\n",
    "        2: 'High School (or A-levels) Diploma',\n",
    "        3: 'Undergraduate degree',\n",
    "        4: 'Postgraduate degree',\n",
    "        5: 'Prefer not to say'\n",
    "    },\n",
    "    'occupation': {\n",
    "        1: 'Artist',\n",
    "        2: 'Civil Engineering',\n",
    "        3: 'Computers & I.T.',\n",
    "        4: 'Director',\n",
    "        5: 'Engineering',\n",
    "        6: 'Entrepreneur',\n",
    "        7: 'Financial Banking',\n",
    "        8: 'Food & Drinks',\n",
    "        9: 'Healthcare',\n",
    "        10: 'Hospitality',\n",
    "        11: 'Legal',\n",
    "        12: 'Leisure',\n",
    "        13: 'Musician',\n",
    "        14: 'Office Administration',\n",
    "        15: 'Other',\n",
    "        16: 'Public Sector',\n",
    "        17: 'Services',\n",
    "        18: 'Publishing & Media',\n",
    "        19: 'Retail',\n",
    "        20: 'Sales',\n",
    "        21: 'Scientific & Technical',\n",
    "        22: 'Supply chain',\n",
    "        23: 'Teaching & Interpretation',\n",
    "        24: 'Transport',\n",
    "        25: 'Other',\n",
    "        26: 'Prefer not to say'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Column mappings created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Data Type Conversion\n",
    "print(\"=== DATA TYPE CONVERSION ===\")\n",
    "\n",
    "# Convert questionnaire columns to numeric\n",
    "questionnaire_cols = spq_cols + eq_cols + sqr_cols + aq_cols + total_cols + zscore_cols\n",
    "for col in questionnaire_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Convert diagnosis columns to numeric\n",
    "for col in diagnosis_cols + autism_diagnosis_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Convert age to numeric\n",
    "if 'age' in df_clean.columns:\n",
    "    df_clean['age'] = pd.to_numeric(df_clean['age'], errors='coerce')\n",
    "\n",
    "print(\"Converted questionnaire, diagnosis, and age columns to numeric\")\n",
    "print(\"\\nUpdated data types:\")\n",
    "print(df_clean.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Conservative Age Cleaning\n",
    "print(\"=== AGE DATA CLEANING ===\")\n",
    "\n",
    "# Check age distribution before cleaning\n",
    "print(\"Age distribution before cleaning:\")\n",
    "print(df_clean['age'].value_counts().sort_index().head(10))\n",
    "print(f\"NaN values: {df_clean['age'].isna().sum()}\")\n",
    "\n",
    "# Only remove rows where age is exactly 0 (likely invalid)\n",
    "initial_count = len(df_clean)\n",
    "age_zero_count = len(df_clean[df_clean['age'] == 0])\n",
    "df_clean = df_clean[df_clean['age'] != 0].copy()\n",
    "final_count = len(df_clean)\n",
    "\n",
    "print(f\"Removed {initial_count - final_count} rows with age = 0\")\n",
    "print(f\"Remaining rows: {final_count}\")\n",
    "\n",
    "# Keep NaN ages - they're just missing data, not invalid\n",
    "print(f\"Rows with missing age: {df_clean['age'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create Autism Diagnosis Indicators\n",
    "def create_autism_indicators(df):\n",
    "    \"\"\"Create autism diagnosis indicators based on diagnosis and autism_diagnosis columns\"\"\"\n",
    "    \n",
    "    # Initialize autism indicators\n",
    "    df['has_autism_diagnosis'] = False\n",
    "    df['autism_diagnosis_type'] = 'None'\n",
    "    \n",
    "    # Check diagnosis columns for autism (value = 2)\n",
    "    diagnosis_cols = [col for col in df.columns if col.startswith('diagnosis_')]\n",
    "    for col in diagnosis_cols:\n",
    "        # If diagnosis_* = 2, that's autism\n",
    "        autism_mask = df[col] == 2\n",
    "        df.loc[autism_mask, 'has_autism_diagnosis'] = True\n",
    "        df.loc[autism_mask, 'autism_diagnosis_type'] = 'Autism Spectrum Disorder'\n",
    "    \n",
    "    # Check autism_diagnosis columns\n",
    "    autism_cols = [col for col in df.columns if col.startswith('autism_diagnosis_')]\n",
    "    for col in autism_cols:\n",
    "        # Values 1, 2, 3 indicate autism diagnosis\n",
    "        autism_mask = df[col].isin([1, 2, 3])\n",
    "        df.loc[autism_mask, 'has_autism_diagnosis'] = True\n",
    "        \n",
    "        # Map autism diagnosis types\n",
    "        df.loc[df[col] == 1, 'autism_diagnosis_type'] = 'Autism (classical autism)'\n",
    "        df.loc[df[col] == 2, 'autism_diagnosis_type'] = 'Asperger Syndrome (AS)'\n",
    "        df.loc[df[col] == 3, 'autism_diagnosis_type'] = 'Other'\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "df_clean = create_autism_indicators(df_clean)\n",
    "\n",
    "# Check results\n",
    "print(\"=== AUTISM DIAGNOSIS SUMMARY ===\")\n",
    "print(f\"Total participants: {len(df_clean)}\")\n",
    "print(f\"Autism diagnosis: {df_clean['has_autism_diagnosis'].sum()} ({df_clean['has_autism_diagnosis'].mean()*100:.2f}%)\")\n",
    "print(\"\\nAutism diagnosis types:\")\n",
    "print(df_clean['autism_diagnosis_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Categorical Variable Exploration\n",
    "def explore_categorical_variables(df, column_mappings):\n",
    "    \"\"\"Explore categorical variables using the provided mappings\"\"\"\n",
    "    \n",
    "    for col, mapping in column_mappings.items():\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n=== {col.upper()} ===\")\n",
    "            value_counts = df[col].value_counts().sort_index()\n",
    "            print(f\"Unique values: {len(value_counts)}\")\n",
    "            print(\"Value counts:\")\n",
    "            for value, count in value_counts.items():\n",
    "                percentage = count/len(df)*100\n",
    "                label = mapping.get(value, f\"Unknown ({value})\")\n",
    "                print(f\"  {value}: {count:,} ({percentage:.2f}%) - {label}\")\n",
    "        else:\n",
    "            print(f\"\\n=== {col.upper()} ===\")\n",
    "            print(\"Column not found in dataset\")\n",
    "\n",
    "print(\"=== CATEGORICAL VARIABLE EXPLORATION ===\")\n",
    "explore_categorical_variables(df_clean, column_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Data Quality Assessment\n",
    "def assess_data_quality(df, column_groups):\n",
    "    \"\"\"Assess data quality for different column groups\"\"\"\n",
    "    \n",
    "    for group_name, columns in column_groups.items():\n",
    "        print(f\"\\n=== {group_name.upper()} QUALITY ASSESSMENT ===\")\n",
    "        \n",
    "        # Filter columns that exist in the dataset\n",
    "        existing_cols = [col for col in columns if col in df.columns]\n",
    "        \n",
    "        if not existing_cols:\n",
    "            print(f\"No {group_name} columns found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        group_df = df[existing_cols]\n",
    "        \n",
    "        # Missing values\n",
    "        missing_pct = (group_df.isnull().sum() / len(group_df)) * 100\n",
    "        print(f\"Missing values (%):\")\n",
    "        for col, pct in missing_pct.items():\n",
    "            if pct > 0:\n",
    "                print(f\"  {col}: {pct:.2f}%\")\n",
    "        \n",
    "        # Data types\n",
    "        print(f\"\\nData types:\")\n",
    "        print(group_df.dtypes.value_counts())\n",
    "        \n",
    "        # For numeric columns, show basic stats\n",
    "        numeric_cols = group_df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(f\"\\nNumeric columns ({len(numeric_cols)}): {list(numeric_cols)}\")\n",
    "            print(\"\\nBasic statistics:\")\n",
    "            display(group_df[numeric_cols].describe())\n",
    "\n",
    "# Define column groups for assessment\n",
    "column_groups = {\n",
    "    'demographics': demographic_cols,\n",
    "    'diagnoses': diagnosis_cols,\n",
    "    'autism_diagnoses': autism_diagnosis_cols,\n",
    "    'spq_scale': spq_cols,\n",
    "    'eq_scale': eq_cols,\n",
    "    'sqr_scale': sqr_cols,\n",
    "    'aq_scale': aq_cols,\n",
    "    'total_scores': total_cols,\n",
    "    'z_scores': zscore_cols,\n",
    "    'derived_variables': derived_cols\n",
    "}\n",
    "\n",
    "assess_data_quality(df_clean, column_groups)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

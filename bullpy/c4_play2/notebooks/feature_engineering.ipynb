{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# feature engineering \n",
    "- matched sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# load matched data \n",
    "df = pd.read_csv('data/processed/data_c4_matched_balanced.csv')\n",
    "\n",
    "# 1. feature creation \n",
    "\n",
    "# age group bins\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
    "\n",
    "#non linear transformation \n",
    "df['log_aq_total'] = np.log1p(df['aq_total'])\n",
    "df['sqrt_age'] = np.sqrt(df['age'])\n",
    "\n",
    "# interaction terms\n",
    "df['aq_eq_interaction'] = df['aq_total'] * df['eq_total']\n",
    "df['sqp_aq_interaction'] = df['spq_total'] * df['aq_total']\n",
    "df['age_x_eq'] = df['age'] * df['eq_total']\n",
    "\n",
    "# questionnaire score ratios \n",
    "df['aq_spq_ratio'] = df['aq_total'] / (df['spq_total'] + 1e-8)\n",
    "df['eq_sqr_ratio'] = df['eq_total'] / (df['sqr_total'] + 1e-8)\n",
    "\n",
    "#boolean: high aq (above 1 std)\n",
    "df['high_aq'] = (df['aq_total'] > df['aq_total'].mean() + df['aq_total'].std()).astype(int)\n",
    "\n",
    "# 2. feature reduction/selection\n",
    "\n",
    "# remove highly correlated features \n",
    "# Only use numeric columns for correlation\n",
    "numeric_cols = df.drop(columns=['autism_target']).select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "df = df.drop(columns=to_drop)\n",
    "\n",
    "# drop low variance features \n",
    "# Only apply VarianceThreshold to numeric columns\n",
    "feature_cols = df.drop(columns=['autism_target']).select_dtypes(include=[np.number]).columns\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selector.fit(df[feature_cols])\n",
    "low_variance_cols = feature_cols[~selector.get_support()]\n",
    "df = df.drop(columns=low_variance_cols)\n",
    "\n",
    "# 3. one-hot encode new categorical features \n",
    "df = pd.get_dummies(df, columns=['age_group'], drop_first=True)\n",
    "\n",
    "# 4. save engineered dataset \n",
    "df.to_csv('data/processed/data_c4_balanced_fe.csv', index=False)\n",
    "\n",
    "print(\"feature engineering complete. new shape:\", df.shape)\n",
    "print(\"columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# df load data\n",
    "df = pd.read_csv('data/processed/data_c4_balanced_fe.csv')\n",
    "x = df.drop(columns=['autism_target'])\n",
    "y = df['autism_target']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_imputed = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_imputed, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# logistic reg\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "logreg.fit(x_train, y_train)\n",
    "print(\"logistic regression:\")\n",
    "print(classification_report(y_val, logreg.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, logreg.predict_proba(x_val)[:, 1]))\n",
    "\n",
    "#random forest \n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"random forest:\")\n",
    "print(classification_report(y_val, rf.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, rf.predict_proba(x_val)[:, 1]))\n",
    "\n",
    "# xgboost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(x_train, y_train)\n",
    "print(\"xgboost:\")\n",
    "print(classification_report(y_val, xgb.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, xgb.predict_proba(x_val)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# threshold moving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, roc_auc_score\n",
    "\n",
    "# Random Forest threshold moving\n",
    "rf_probs = rf.predict_proba(x_val)[:, 1]\n",
    "\n",
    "rf_prec, rf_rec, rf_thresholds = precision_recall_curve(y_val, rf_probs)\n",
    "rf_f1s = 2 * (rf_prec * rf_rec) / (rf_prec + rf_rec + 1e-8)\n",
    "rf_best_thresh = rf_thresholds[np.argmax(rf_f1s)]\n",
    "print(f\"Random Forest - Best threshold for F1: {rf_best_thresh:.3f}\")\n",
    "\n",
    "# evaluate RF at the best threshold \n",
    "rf_pred_thresh = (rf_probs >= rf_best_thresh).astype(int)\n",
    "print(\"Random Forest validation set performance at best threshold:\")\n",
    "print(classification_report(y_val, rf_pred_thresh))\n",
    "print(f\"F1 at best threshold: {f1_score(y_val, rf_pred_thresh):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, rf_probs):.3f}\")\n",
    "\n",
    "# XGBoost threshold moving\n",
    "xgb_probs = xgb.predict_proba(x_val)[:, 1]\n",
    "\n",
    "xgb_prec, xgb_rec, xgb_thresholds = precision_recall_curve(y_val, xgb_probs)\n",
    "xgb_f1s = 2 * (xgb_prec * xgb_rec) / (xgb_prec + xgb_rec + 1e-8)\n",
    "xgb_best_thresh = xgb_thresholds[np.argmax(xgb_f1s)]\n",
    "print(f\"\\nXGBoost - Best threshold for F1: {xgb_best_thresh:.3f}\")\n",
    "\n",
    "# evaluate XGBoost at the best threshold \n",
    "xgb_pred_thresh = (xgb_probs >= xgb_best_thresh).astype(int)\n",
    "print(\"XGBoost validation set performance at best threshold:\")\n",
    "print(classification_report(y_val, xgb_pred_thresh))\n",
    "print(f\"F1 at best threshold: {f1_score(y_val, xgb_pred_thresh):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, xgb_probs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# for random forest\n",
    "importances = pd.Series(rf.feature_importances_, index=x_train.columns)\n",
    "print(\"Top 20 random forest features:\")\n",
    "print(importances.sort_values(ascending=False).head(20))\n",
    "\n",
    "# for xgboost\n",
    "importances_xgb = pd.Series(xgb.feature_importances_, index=x_train.columns)\n",
    "print(\"Top 20 xgboost features:\")\n",
    "print(importances_xgb.sort_values(ascending=False).head(20))\n",
    "\n",
    "# shap for interpretable analysis\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# for RF - using a subsample for faster SHAP computation\n",
    "# Take a smaller sample (e.g., 20% of validation data) for efficiency\n",
    "sample_size = min(500, int(0.2 * len(x_val)))\n",
    "x_val_sample = x_val.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Create explainer and compute SHAP values only on the subsample\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(x_val_sample)\n",
    "\n",
    "# Generate plots with the subsampled data\n",
    "shap.summary_plot(shap_values, x_val_sample, plot_type=\"bar\", max_display=20)\n",
    "shap.summary_plot(shap_values, x_val_sample, max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# try other local models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(rf, x_val, y_val, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "perm_importances = pd.Series(result.importances_mean, index=x_val.columns)\n",
    "print(\"Top 20 permutation importances:\")\n",
    "print(perm_importances.sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple ensembling\n",
    "# get probabilities from both models \n",
    "probs_logreg = logreg.predict_proba(x_val)[:, 1]\n",
    "probs_rf = rf.predict_proba(x_val)[:, 1]\n",
    "\n",
    "# simple average ensemble \n",
    "ensemble_probs = (probs_logreg + probs_rf) / 2\n",
    "ensemble_pred = (ensemble_probs >= 0.5).astype(int)\n",
    "print(\"ensemble performance:\")\n",
    "print(classification_report(y_val, ensemble_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, ensemble_probs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real test data\n",
    "df_real = pd.read_csv('data/processed/data_c4_processed.csv')\n",
    "X_real = df_real[x_train.columns]  # Use same features as in training\n",
    "y_real = df_real['autism_target']\n",
    "\n",
    "# Use the trained random forest model to make predictions\n",
    "probs_real = rf.predict_proba(X_real)[:, 1]\n",
    "y_pred_real = (probs_real >= 0.35).astype(int)\n",
    "print(classification_report(y_real, y_pred_real))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_real, probs_real))\n",
    "\n",
    "# Try with optimized threshold (0.35)\n",
    "y_pred_real_opt = (probs_real >= 0.35).astype(int)\n",
    "print(\"\\nWith optimized threshold (0.35):\")\n",
    "print(classification_report(y_real, y_pred_real_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune threshold on real data\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Use X_real instead of x_real to match the variable name defined in cell 12\n",
    "probs_real = rf.predict_proba(X_real)[:, 1]\n",
    "prec, rec, thresholds = precision_recall_curve(y_real, probs_real)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "\n",
    "best_thresh_real = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best threshold for F1 on real data: {best_thresh_real:.3f}\")\n",
    "\n",
    "y_pred_real_best = (probs_real >= best_thresh_real).astype(int)\n",
    "print(classification_report(y_real, y_pred_real_best))\n",
    "print(\"F1 at best threshold:\", f1_score(y_real, y_pred_real_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# testing threshold on real test train validation split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune threshold on validation set \n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, roc_auc_score\n",
    "\n",
    "probs_val = rf.predict_proba(x_val)[:, 1]  # <-- fixed here\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, probs_val)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "best_thresh_val = thresholds[np.argmax(f1s)]\n",
    "print(f\"Best threshold for F1 on val set: {best_thresh_val:.3f}\")\n",
    "\n",
    "y_pred_val_best = (probs_val >= best_thresh_val).astype(int)\n",
    "print(\"Validation set performance at best threshold:\")\n",
    "print(classification_report(y_val, y_pred_val_best))\n",
    "print(f\"F1 at best threshold: {f1_score(y_val, y_pred_val_best):.3f}\")\n",
    "\n",
    "# evaluate on test set \n",
    "probs_test = rf.predict_proba(x_test)[:, 1]\n",
    "y_pred_test = (probs_test >= best_thresh_val).astype(int)\n",
    "print(\"Test set performance at validation-optimized threshold:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"F1 at best threshold:\", f1_score(y_test, y_pred_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, probs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# checking for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_real.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for suspiciously high correlations\n",
    "import numpy as np\n",
    "correlations = [np.corrcoef(X_real[col], y_real)[0,1] for col in X_real.columns]\n",
    "for col, corr in zip(X_real.columns, correlations):\n",
    "    if abs(corr) > 0.95:\n",
    "        print(f\"Suspiciously high correlation: {col} ({corr:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: training simple model on real data to ensure the features engineering is just good\n",
    "# result: This is a good sign: your features have real predictive power, and the model can rank cases well.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Split your real data (if not already split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_real, y_real, test_size=0.2, stratify=y_real, random_state=42)\n",
    "\n",
    "# Train a simple logistic regression\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = logreg.predict(X_test)\n",
    "probs = logreg.predict_proba(X_test)[:, 1]\n",
    "print(\"Logistic Regression performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log reg with improved convergence and feature scaling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with scaling to help with convergence\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(solver='liblinear', max_iter=10000))\n",
    "])\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_grid = {\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logreg__class_weight': [None, 'balanced'],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear', 'saga']  # These solvers support both L1 and L2\n",
    "}\n",
    "\n",
    "# Use more cross-validation folds for better estimates\n",
    "grid = GridSearchCV(pipe, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "grid.fit(x_train, y_train)\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "pprint(grid.best_params_)\n",
    "print(f\"Best F1 score: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(x_val)\n",
    "print(\"\\nValidation set performance:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, best_model.predict_proba(x_val)[:, 1]):.3f}\")\n",
    "\n",
    "# Print all results as a DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "top_results = results_df.sort_values('mean_test_score', ascending=False).head(5)\n",
    "print(top_results[['params', 'mean_test_score', 'std_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded parameter grid for Random Forest with more options\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300, 500],  # Try more trees\n",
    "    'rf__max_depth': [8, 10, 15, 20, None],  # More granular depth options around best value\n",
    "    'rf__min_samples_split': [2, 5, 10],  # Add higher value\n",
    "    'rf__min_samples_leaf': [1, 2, 4],  # Control leaf size\n",
    "    'rf__max_features': ['sqrt', 'log2', None],  # Try different feature selection strategies\n",
    "    'rf__class_weight': [None, 'balanced', 'balanced_subsample'],  # Add balanced_subsample\n",
    "    'rf__bootstrap': [True, False]  # Try with and without bootstrap\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling for Random Forest\n",
    "rf_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add scaling which might help\n",
    "    ('rf', RandomForestClassifier(random_state=42))  # Add random_state for reproducibility\n",
    "])\n",
    "\n",
    "# Use 5-fold CV for more robust estimates\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "print(\"RF best params:\", rf_grid.best_params_)\n",
    "print(f\"RF best F1: {rf_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_best_model = rf_grid.best_estimator_\n",
    "rf_y_pred = rf_best_model.predict(x_val)\n",
    "print(\"\\nRF Validation set performance:\")\n",
    "print(classification_report(y_val, rf_y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, rf_best_model.predict_proba(x_val)[:, 1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more extensive parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 300, 500],  # Try more trees\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 8, 10],      # More granular depth options\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1, 0.2],  # More learning rate options\n",
    "    'xgb__min_child_weight': [1, 3, 5],         # Control overfitting\n",
    "    'xgb__gamma': [0, 0.1, 0.2],                # Minimum loss reduction for partition\n",
    "    'xgb__subsample': [0.8, 0.9, 1.0],          # Fraction of samples for trees\n",
    "    'xgb__colsample_bytree': [0.8, 0.9, 1.0],   # Fraction of features for trees\n",
    "    'xgb__reg_alpha': [0, 0.1, 1],              # L1 regularization\n",
    "    'xgb__reg_lambda': [1, 1.5, 2],             # L2 regularization\n",
    "    'xgb__scale_pos_weight': [1, sum(y_train==0)/sum(y_train==1)]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Create pipeline with preprocessing\n",
    "xgb_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add scaling which might help\n",
    "    ('xgb', XGBClassifier(eval_metric='logloss', verbosity=0))  # Remove use_label_encoder, add verbosity=0\n",
    "])\n",
    "\n",
    "# Use 5-fold CV for more robust estimates\n",
    "xgb_grid = GridSearchCV(xgb_pipe, xgb_param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "print(\"XGB best params:\", xgb_grid.best_params_)\n",
    "print(f\"XGB best F1: {xgb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "xgb_best_model = xgb_grid.best_estimator_\n",
    "xgb_y_pred = xgb_best_model.predict(x_val)\n",
    "print(\"\\nXGB Validation set performance:\")\n",
    "print(classification_report(y_val, xgb_y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, xgb_best_model.predict_proba(x_val)[:, 1]):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (c4_play2)",
   "language": "python",
   "name": "c4_play2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

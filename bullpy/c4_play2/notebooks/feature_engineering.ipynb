{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# feature engineering \n",
    "- matched sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# load matched data \n",
    "df = pd.read_csv('data/processed/data_c4_matched_balanced.csv')\n",
    "\n",
    "# 1. feature creation \n",
    "\n",
    "# age group bins\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 18, 30, 45, 60, 100], labels=['0-18', '19-30', '31-45', '46-60', '61+'])\n",
    "\n",
    "#non linear transformation \n",
    "df['log_aq_total'] = np.log1p(df['aq_total'])\n",
    "df['sqrt_age'] = np.sqrt(df['age'])\n",
    "\n",
    "# interaction terms\n",
    "df['aq_eq_interaction'] = df['aq_total'] * df['eq_total']\n",
    "df['sqp_aq_interaction'] = df['spq_total'] * df['aq_total']\n",
    "df['age_x_eq'] = df['age'] * df['eq_total']\n",
    "\n",
    "# questionnaire score ratios \n",
    "df['aq_spq_ratio'] = df['aq_total'] / (df['spq_total'] + 1e-8)\n",
    "df['eq_sqr_ratio'] = df['eq_total'] / (df['sqr_total'] + 1e-8)\n",
    "\n",
    "#boolean: high aq (above 1 std)\n",
    "df['high_aq'] = (df['aq_total'] > df['aq_total'].mean() + df['aq_total'].std()).astype(int)\n",
    "\n",
    "# 2. feature reduction/selection\n",
    "\n",
    "# remove highly correlated features \n",
    "# Only use numeric columns for correlation\n",
    "numeric_cols = df.drop(columns=['autism_target']).select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df[numeric_cols].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "df = df.drop(columns=to_drop)\n",
    "\n",
    "# drop low variance features \n",
    "# Only apply VarianceThreshold to numeric columns\n",
    "feature_cols = df.drop(columns=['autism_target']).select_dtypes(include=[np.number]).columns\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selector.fit(df[feature_cols])\n",
    "low_variance_cols = feature_cols[~selector.get_support()]\n",
    "df = df.drop(columns=low_variance_cols)\n",
    "\n",
    "# 3. one-hot encode new categorical features \n",
    "df = pd.get_dummies(df, columns=['age_group'], drop_first=True)\n",
    "\n",
    "# 4. save engineered dataset \n",
    "df.to_csv('data/processed/data_c4_balanced_fe.csv', index=False)\n",
    "\n",
    "print(\"feature engineering complete. new shape:\", df.shape)\n",
    "print(\"columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# df load data\n",
    "df = pd.read_csv('data/processed/data_c4_balanced_fe.csv')\n",
    "x = df.drop(columns=['autism_target'])\n",
    "y = df['autism_target']\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_imputed = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_imputed, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# logistic reg\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "logreg.fit(x_train, y_train)\n",
    "print(\"logistic regression:\")\n",
    "print(classification_report(y_val, logreg.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, logreg.predict_proba(x_val)[:, 1]))\n",
    "\n",
    "#random forest \n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"random forest:\")\n",
    "print(classification_report(y_val, rf.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, rf.predict_proba(x_val)[:, 1]))\n",
    "\n",
    "# xgboost\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(x_train, y_train)\n",
    "print(\"xgboost:\")\n",
    "print(classification_report(y_val, xgb.predict(x_val)))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, xgb.predict_proba(x_val)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log reg with improved convergence and feature scaling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with scaling to help with convergence\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(solver='liblinear', max_iter=10000))\n",
    "])\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_grid = {\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logreg__class_weight': [None, 'balanced'],\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__solver': ['liblinear', 'saga']  # These solvers support both L1 and L2\n",
    "}\n",
    "\n",
    "# Use more cross-validation folds for better estimates\n",
    "grid = GridSearchCV(pipe, param_grid, scoring='f1', cv=5, n_jobs=-1)\n",
    "grid.fit(x_train, y_train)\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "pprint(grid.best_params_)\n",
    "print(f\"Best F1 score: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(x_val)\n",
    "print(\"\\nValidation set performance:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, best_model.predict_proba(x_val)[:, 1]):.3f}\")\n",
    "\n",
    "# Print all results as a DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "top_results = results_df.sort_values('mean_test_score', ascending=False).head(5)\n",
    "print(top_results[['params', 'mean_test_score', 'std_test_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded parameter grid for Random Forest with more options\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [100, 200, 300, 500],  # Try more trees\n",
    "    'rf__max_depth': [8, 10, 15, 20, None],  # More granular depth options around best value\n",
    "    'rf__min_samples_split': [2, 5, 10],  # Add higher value\n",
    "    'rf__min_samples_leaf': [1, 2, 4],  # Control leaf size\n",
    "    'rf__max_features': ['sqrt', 'log2', None],  # Try different feature selection strategies\n",
    "    'rf__class_weight': [None, 'balanced', 'balanced_subsample'],  # Add balanced_subsample\n",
    "    'rf__bootstrap': [True, False]  # Try with and without bootstrap\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling for Random Forest\n",
    "rf_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add scaling which might help\n",
    "    ('rf', RandomForestClassifier(random_state=42))  # Add random_state for reproducibility\n",
    "])\n",
    "\n",
    "# Use 5-fold CV for more robust estimates\n",
    "rf_grid = GridSearchCV(rf_pipe, rf_param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(x_train, y_train)\n",
    "print(\"RF best params:\", rf_grid.best_params_)\n",
    "print(f\"RF best F1: {rf_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_best_model = rf_grid.best_estimator_\n",
    "rf_y_pred = rf_best_model.predict(x_val)\n",
    "print(\"\\nRF Validation set performance:\")\n",
    "print(classification_report(y_val, rf_y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, rf_best_model.predict_proba(x_val)[:, 1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more extensive parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'xgb__n_estimators': [100, 200, 300, 500],  # Try more trees\n",
    "    'xgb__max_depth': [3, 4, 5, 6, 8, 10],      # More granular depth options\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1, 0.2],  # More learning rate options\n",
    "    'xgb__min_child_weight': [1, 3, 5],         # Control overfitting\n",
    "    'xgb__gamma': [0, 0.1, 0.2],                # Minimum loss reduction for partition\n",
    "    'xgb__subsample': [0.8, 0.9, 1.0],          # Fraction of samples for trees\n",
    "    'xgb__colsample_bytree': [0.8, 0.9, 1.0],   # Fraction of features for trees\n",
    "    'xgb__reg_alpha': [0, 0.1, 1],              # L1 regularization\n",
    "    'xgb__reg_lambda': [1, 1.5, 2],             # L2 regularization\n",
    "    'xgb__scale_pos_weight': [1, sum(y_train==0)/sum(y_train==1)]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Create pipeline with preprocessing\n",
    "xgb_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add scaling which might help\n",
    "    ('xgb', XGBClassifier(eval_metric='logloss', verbosity=0))  # Remove use_label_encoder, add verbosity=0\n",
    "])\n",
    "\n",
    "# Use 5-fold CV for more robust estimates\n",
    "xgb_grid = GridSearchCV(xgb_pipe, xgb_param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(x_train, y_train)\n",
    "print(\"XGB best params:\", xgb_grid.best_params_)\n",
    "print(f\"XGB best F1: {xgb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "xgb_best_model = xgb_grid.best_estimator_\n",
    "xgb_y_pred = xgb_best_model.predict(x_val)\n",
    "print(\"\\nXGB Validation set performance:\")\n",
    "print(classification_report(y_val, xgb_y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, xgb_best_model.predict_proba(x_val)[:, 1]):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (c4_play2)",
   "language": "python",
   "name": "c4_play2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 750k dataset \n",
    "- somebody call dora \n",
    "- inital play and exploration, run full tuning and optimisation on hpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#make plots leng \n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# load data \n",
    "df = pd.read_csv('/Users/eb2007/Library/CloudStorage/OneDrive-UniversityofCambridge/Documents/PhD/data/data_c4_raw.csv')\n",
    "\n",
    "# initial data inspection \n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"columns:\", df.columns)\n",
    "display(df.head())\n",
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# basic stats \n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# visualise the data \n",
    "# distribution of target variable\n",
    "# what are the values in columns\n",
    "print(\"sample of diagnosis columns\")\n",
    "diagnosis_cols = [col for col in df.columns if 'diagnosis' in col]\n",
    "print(diagnosis_cols)\n",
    "\n",
    "# any autism diagnosis \n",
    "autism_cols = [col for col in df.columns if 'autism_diagnosis' in col]\n",
    "\n",
    "df['autism_any'] = df[autism_cols].apply(\n",
    "    lambda row: int(any(x in [1.0, 2.0, 3.0] for x in row if not pd.isnull(x))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df['autism_any'].value_counts())\n",
    "sns.countplot(x='autism_any', data=df)\n",
    "plt.title('any autism diagnosis (0 no 1 yes)')\n",
    "plt.show()\n",
    "\n",
    "# multi class target: most specific autism subtype \n",
    "def get_first_autism_subtype(row):\n",
    "    for x in row :\n",
    "        if x in [1.0, 2.0, 3.0]:\n",
    "            return int(x)\n",
    "    return 0 # no autism diagnosis \n",
    "\n",
    "df['autism_subtype'] = df[autism_cols].apply(get_first_autism_subtype, axis=1)\n",
    "print(df['autism_subtype'].value_counts())\n",
    "sns.countplot(x='autism_subtype', data=df)\n",
    "plt.title('Autism subtype')\n",
    "plt.show()\n",
    "\n",
    "#multi-label: one hot encoding for each subtype\n",
    "\n",
    "#create seperate columns for each subtype \n",
    "for subtype in [1.0, 2.0, 3.0]:\n",
    "    df[f'autism_subtype_{int(subtype)}'] = df[autism_cols].apply(\n",
    "        lambda row: int(subtype in row.values), axis=1\n",
    "    )\n",
    "\n",
    "print(df[[f'autism_subtype_{i}' for i in [1, 2, 3]]].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionaries for your coded variables\n",
    "\n",
    "sex_map = {1: 'Male', 2: 'Female', 3: 'Transgender/Other', 4: 'Prefer not to say'}\n",
    "handedness_map = {1: 'Right-handed', 2: 'Left-handed', 3: 'Ambidextrous', 4: 'Prefer not to say'}\n",
    "education_map = {\n",
    "    1: 'Did not complete High School (or A-levels)',\n",
    "    2: 'High School (or A-levels) Diploma',\n",
    "    3: 'Undergraduate degree',\n",
    "    4: 'Postgraduate degree',\n",
    "    5: 'Prefer not to say'\n",
    "}\n",
    "occupation_map = {\n",
    "    1: 'Artist', 2: 'Civil Engineering', 3: 'Computers & I.T.', 4: 'Director', 5: 'Engineering',\n",
    "    6: 'Entrepreneur', 7: 'Financial Banking', 8: 'Food & Drinks', 9: 'Healthcare', 10: 'Hospitality',\n",
    "    11: 'Legal', 12: 'Leisure', 13: 'Musician', 14: 'Office Administration', 15: 'Other',\n",
    "    16: 'Public Sector', 17: 'Services', 18: 'Publishing & Media', 19: 'Retail', 20: 'Sales',\n",
    "    21: 'Scientific & Technical', 22: 'Supply chain', 23: 'Teaching & Interpretation', 24: 'Transport',\n",
    "    25: 'Other', 26: 'Prefer not to say'\n",
    "}\n",
    "region_map = {\n",
    "    1: 'Wales', 2: 'Scotland', 3: 'Northern Ireland', 4: 'London (England)', 5: 'North East (England)',\n",
    "    6: 'North West (England)', 7: 'Yorkshire and Humber (England)', 8: 'West Midlands (England)',\n",
    "    9: 'East Midlands (England)', 10: 'South East (England)', 11: 'South West (England)',\n",
    "    12: 'Other (outside of the United Kingdom)', 13: 'Other (in the United Kingdom)', 14: 'Prefer not to say'\n",
    "}\n",
    "country_region_map = {\n",
    "    1: 'Wales', 2: 'Scotland', 3: 'Northern Ireland', 4: 'London (England)', 5: 'North East (England)',\n",
    "    6: 'North West (England)', 7: 'Yorkshire and Humber (England)', 8: 'West Midlands (England)',\n",
    "    9: 'East Midlands (England)', 10: 'South East (England)', 11: 'South West (England)',\n",
    "    12: 'Other (outside of the United Kingdom)', 13: 'Other (in the United Kingdom)', 14: 'Prefer not to say'\n",
    "}\n",
    "diagnosis_map = {\n",
    "    1: 'ADHD', 2: 'Autism Spectrum Disorder', 3: 'Bipolar Disorder', 4: 'Depression',\n",
    "    5: 'Learning disability', 6: 'OCD', 7: 'Schizophrenia', 8: 'Prefer not to say',\n",
    "    9: 'No diagnosis'\n",
    "}\n",
    "asd_map = {1: 'Autism (classical autism)', 2: 'Asperger Syndrome (AS)', 3: 'Other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting distribution for each subtype \n",
    "for i in [1 ,2 ,3]:\n",
    "    sns.countplot(x=f'autism_subtype_{i}', data=df)\n",
    "    plt.title(f'Autism subtype {i} (0 no 1 yes)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EDA feature overview and missing data ---\n",
    "\n",
    "# list all columns and their types \n",
    "print(df.dtypes)\n",
    "\n",
    "# count missing values per column \n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values per column:\\n\", missing[missing > 0])\n",
    "\n",
    "# visualize missing data \n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Univariate Analysis: categorical features ---\n",
    "\n",
    "# list of known coded categorical columns \n",
    "coded_cat_cols = ['sex', 'handedness', 'education', 'occupation', 'country_region']\n",
    "\n",
    "for col in coded_cat_cols:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        df[col].value_counts(dropna=False).sort_index().plot(kind='bar')\n",
    "        plt.title(f'Values counts of {col}')\n",
    "        plt.xlabel(f'{col} (coded)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- print value counts with labels for demographic columns ---\n",
    "\n",
    "coded_cat_cols = [\n",
    "    ('sex', sex_map),\n",
    "    ('handedness', handedness_map),\n",
    "    ('education', education_map),\n",
    "    ('occupation', occupation_map),\n",
    "    ('country_region', country_region_map),\n",
    "    ('diagnosis', diagnosis_map),\n",
    "    ('asd', asd_map),\n",
    "]\n",
    "\n",
    "for col, mapping in coded_cat_cols:\n",
    "    if col in df.columns:\n",
    "        counts = df[col].value_counts(dropna=False).sort_index()\n",
    "        print(f\"\\nValue counts for {col}:\")\n",
    "        for code, count in counts.items():\n",
    "            label = mapping.get(int(code), 'Missing/Unknown') if pd.notnull(code) else 'missing'\n",
    "            print(f\" {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- print total value counts for all diagnosis columns ---\n",
    "from collections import Counter \n",
    "\n",
    "#gather all values from all diagnosis columns\n",
    "all_diagnosis_values = []\n",
    "for col in diagnosis_cols:\n",
    "    all_diagnosis_values.extend(df[col].dropna().astype(int).tolist())\n",
    "\n",
    "# count occurrences \n",
    "diagnosis_totals = Counter(all_diagnosis_values)\n",
    "\n",
    "print(\"\\nTotal counts for each diagnosis (across all columns):\")\n",
    "for code, count in diagnosis_totals.items():\n",
    "    label = diagnosis_map.get(code, f'Unknown ({code})')\n",
    "    print(f' {label}: {count}')\n",
    "\n",
    "# print value counts for all asd columns \n",
    "asd_cols = [col for col in df.columns if col.startswith('autism_diagnosis_')]\n",
    "for col in asd_cols:\n",
    "    counts = df[col].value_counts(dropna=False).sort_index()\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    for code, count in counts.items():\n",
    "        label = asd_map.get(int(code), 'Missing/Unknown') if pd.notnull(code) else 'missing'\n",
    "        print(f' {label}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values, hybird approach, impute missing values and if too many then drop rows with missing values  \n",
    "\n",
    "#define column groups \n",
    "spq_cols = [f'spq_{i}' for i in range(1, 11)]\n",
    "eq_cols = [f'eq_{i}' for i in range(1, 11)]\n",
    "sqr_cols = [f'sqr_{i}' for i in range(1, 11)]\n",
    "aq_cols = [f'aq_{i}' for i in range(1, 11)]\n",
    "age_col = ['age']\n",
    "sex_col = ['sex']\n",
    "handedness_col = ['handedness']\n",
    "education_col = ['education']\n",
    "occupation_col = ['occupation']\n",
    "country_region_col = ['country_region']\n",
    "repeat_col = ['repeat']\n",
    "autism_diagnosis_cols = [f'autism_diagnosis_{i}' for i in range(0, 3)]\n",
    "\n",
    "# summary table\n",
    "for block_names, block_cols in zip(\n",
    "    ['SPQ', 'EQ', 'SQR', 'AQ', 'AGE', 'SEX', 'HANDEDNESS', 'EDUCATION', 'OCCUPATION', 'COUNTRY_REGION', 'REPEAT', 'DIAGNOSIS_0', 'DIAGNOSIS_1', 'DIAGNOSIS_2', 'DIAGNOSIS_3', 'DIAGNOSIS_4', 'DIAGNOSIS_5', 'DIAGNOSIS_6', 'DIAGNOSIS_7', 'DIAGNOSIS_8', 'AUTISM_DIAGNOSIS_0', 'AUTISM_DIAGNOSIS_1', 'AUTISM_DIAGNOSIS_2'],\n",
    "    [spq_cols, eq_cols, sqr_cols, aq_cols, age_col, sex_col, handedness_col, education_col, occupation_col, country_region_col, repeat_col, diagnosis_cols, autism_diagnosis_cols]\n",
    "):\n",
    "    total_missing = df[block_cols].isnull().sum().sum()\n",
    "\n",
    "# print missing values in OG df\n",
    "print(\"Missing values per column:\")  # Fixed typo in \"values\"\n",
    "print(df.isnull().sum().sort_values(ascending=False))\n",
    "print(df[['sex', 'handedness', 'education', 'occupation', 'country_region', 'repeat']].isnull().sum())\n",
    "\n",
    "# check for non standard missing values \n",
    "for col in spq_cols:\n",
    "    print(f\"{col}: unique values: {df[col].unique()}\")\n",
    "for col in eq_cols:\n",
    "    print(f\"{col}: unique values: {df[col].unique()}\")\n",
    "for col in sqr_cols:\n",
    "    print(f\"{col}: unique values: {df[col].unique()}\")\n",
    "for col in aq_cols:\n",
    "    print(f\"{col}: unique values: {df[col].unique()}\")\n",
    "# convert non standard missing values to NaN \n",
    "df[spq_cols] = df[spq_cols].replace([-1, 999, ''], np.nan)\n",
    "df[eq_cols] = df[eq_cols].replace([-1, 999, ''], np.nan)\n",
    "df[sqr_cols] = df[sqr_cols].replace([-1, 999, ''], np.nan)\n",
    "df[aq_cols] = df[aq_cols].replace([-1, 999, ''], np.nan)\n",
    "df[age_col] = df[age_col].replace(-1, np.nan)\n",
    "df[sex_col] = df[sex_col].replace(-1, np.nan)\n",
    "df[handedness_col] = df[handedness_col].replace(-1, np.nan)\n",
    "\n",
    "# wrapping the hybird approach in a function \n",
    "def hybrid_impute_drop(df, cols, max_missing=2, strategy='mean'):\n",
    "    missing_counts = df[cols].isnull().sum(axis=1)\n",
    "    to_impute = df[missing_counts <= max_missing].copy()\n",
    "    to_drop = df[missing_counts > max_missing].copy()\n",
    "    if strategy == 'mean':\n",
    "        to_impute[cols] = to_impute[cols].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
    "    elif strategy == 'median':\n",
    "        to_impute[cols] = to_impute[cols].apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "        # add more strats if needed\n",
    "    return to_impute\n",
    "\n",
    "# example usage \n",
    "df_aq_clean = hybrid_impute_drop(df, [f'aq_{i}' for i in range(1, 11)], max_missing=2, strategy='mean')\n",
    "print(\"AQ: original no. of rows:\", df.shape[0])\n",
    "print(\"number of rows after cleaning:\", df_aq_clean.shape[0])\n",
    "print(\"missing alues per column after cleaning:\")\n",
    "print(df_aq_clean[[f'aq_{i}' for i in range(1, 11)]].isnull().sum())\n",
    "\n",
    "# impute missing values with a new category \n",
    "for col in ['sex', 'age','handedness', 'education', 'occupation', 'country_region', 'repeat']:\n",
    "    df[col] = df[col].fillna(0) # 0 is the new category \n",
    "    print(f\"{col}: unique values: {df[col].unique()}\")\n",
    "    print(f\"{col}: missing values: {df[col].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing Steps\n",
    "\n",
    "1. Checked for missing values across all columns and specifically for demographic variables (sex, handedness, education, occupation, country_region, repeat)\n",
    "\n",
    "2. Inspected unique values in questionnaire columns:\n",
    "   - SPQ (Schizotypal Personality Questionnaire)\n",
    "   - EQ (Empathy Quotient)\n",
    "   - SQR (Systemizing Quotient Revised)\n",
    "   - AQ (Autism Quotient)\n",
    "\n",
    "3. Standardized missing value handling:\n",
    "   - Converted non-standard missing values (-1, 999, '') to NaN for all questionnaire columns\n",
    "   - Also standardized missing values for demographic variables (age, sex, handedness)\n",
    "\n",
    "4. Implemented a hybrid approach for handling missing questionnaire data:\n",
    "   - Created function `hybrid_impute_drop()` that:\n",
    "     - Keeps and imputes rows with ≤ 2 missing values\n",
    "     - Drops rows with > 2 missing values\n",
    "     - Supports mean and median imputation strategies\n",
    "\n",
    "5. Applied the hybrid approach to AQ data:\n",
    "   - Used mean imputation\n",
    "   - Verified the cleaning results by comparing original vs cleaned row counts\n",
    "   - Checked remaining missing values per column\n",
    "\n",
    "6. Handled missing demographic data:\n",
    "   - Imputed missing values with a new category (0)\n",
    "   - Verified the imputation for sex, age, handedness, education, occupation, country_region, and repeat variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert category to dtype \n",
    "cat_cols = ['sex', 'handedness', 'education', 'occupation', 'country_region', 'repeat']\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[col] = df[col].cat.codes\n",
    "\n",
    "# one hot encoding for linear models \n",
    "cat_cols = ['sex', 'handedness', 'education', 'occupation', 'country_region', 'repeat']\n",
    "df_onehot = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# creating composite scores \n",
    "df['spq_total'] = df[[f'spq_{i}' for i in range(1, 11)]].sum(axis=1)\n",
    "df['eq_total'] = df[[f'eq_{i}' for i in range(1, 11)]].sum(axis=1)\n",
    "df['sqr_total'] = df[[f'sqr_{i}' for i in range(1, 11)]].sum(axis=1)\n",
    "df['aq_total'] = df[[f'aq_{i}' for i in range(1, 11)]].sum(axis=1)\n",
    "\n",
    "# create a new column for the total score \n",
    "df['total_score'] = df['spq_total'] + df['eq_total'] + df['sqr_total'] + df['aq_total']\n",
    "\n",
    "#aggregating diagnosis columns \n",
    "diagnosis_cols = [f'diagnosis_{i}' for i in range(9)]\n",
    "df['num_diagnoses'] = df[diagnosis_cols].notnull().sum(axis=1)\n",
    "\n",
    "# binary flag for any ADHD diagnosis\n",
    "df['has_adhd'] = df[diagnosis_cols].apply(lambda row: 1 if 1 in row.values else 0, axis=1)\n",
    "\n",
    "#check encoding \n",
    "print(df[cat_cols].head())\n",
    "print(df[cat_cols].dtypes)\n",
    "print(df_onehot.head())\n",
    "print(df_onehot.dtypes)\n",
    "\n",
    "# check for missing values \n",
    "print(df.isnull().sum())\n",
    "print(df_onehot.isnull().sum())\n",
    "\n",
    "# check for duplicates \n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# check composite scores\n",
    "print(df[['spq_total', 'eq_total', 'sqr_total', 'aq_total', 'total_score', 'num_diagnoses', 'has_adhd']].describe())\n",
    "print(df['num_diagnoses'].describe())\n",
    "print(df['has_adhd'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Converted categorical variables (sex, handedness, education, occupation, country_region, repeat) to numeric codes\n",
    "- Created one-hot encoded version of categorical variables for linear models\n",
    "- Generated composite scores by summing individual question responses:\n",
    "  - SPQ (Sensory Perception Questionnaire) total\n",
    "  - EQ (Empathy Quotient) total  \n",
    "  - SQR (Systemizing Quotient Revised) total\n",
    "  - AQ (Autism Spectrum Quotient) total\n",
    "- Created overall total score combining all questionnaire scores\n",
    "- Aggregated diagnosis columns to create:\n",
    "  - Count of total diagnoses per person\n",
    "  - Binary flag for ADHD diagnosis presence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# train_test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#list of cols to drop in this df to ensure there is no leakage (cheating)\n",
    "cols_to_drop = [\n",
    "    'autism_any', 'userid',\n",
    "    # Drop all autism subtype columns\n",
    "    'autism_subtype', 'autism_subtype_1', 'autism_subtype_2', 'autism_subtype_3',\n",
    "    # Drop all autism diagnosis columns\n",
    "    'autism_diagnosis_0', 'autism_diagnosis_1', 'autism_diagnosis_2'\n",
    "    # Add any other columns you know are derived from the target\n",
    "]\n",
    "\n",
    "#features and target for tree-based models \n",
    "x_tree = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "y = df['autism_any']\n",
    "\n",
    "#features for linear models (one-hot encoded)\n",
    "x_linear = df_onehot.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "#split (use same randome_state for reproducibility)\n",
    "x_train_tree, x_test_tree, y_train, y_test = train_test_split(\n",
    "    x_tree, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "x_train_linear, x_test_linear, _, _ = train_test_split(\n",
    "    x_linear, y, test_size=0.2, random_state=42, stratify=y \n",
    ") #stratify=y ensures class balance preserved in both splits \n",
    "\n",
    "#scaling for linear/neural models \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_linear_scaled = scaler.fit_transform(x_train_linear)\n",
    "x_test_linear_scaled = scaler.transform(x_test_linear)\n",
    "\n",
    "# check the shapes of the splits \n",
    "print(\"Tree-based features train shape:\", x_train_tree.shape)\n",
    "print(\"Tree-based features test shape:\", x_test_tree.shape)\n",
    "print(\"Linear features train shape:\", x_train_linear.shape)\n",
    "print(\"Linear features test shape:\", x_test_linear.shape)\n",
    "print(\"Scaled linear train shape:\", x_train_linear_scaled.shape)\n",
    "print(\"Scaled linear test shape:\", x_test_linear_scaled.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Train Test Split Summary\n",
    "- Created separate feature sets for tree-based models (x_tree) and linear models (x_linear with one-hot encoding)\n",
    "- Removed potential data leakage by dropping autism-related diagnosis/subtype columns\n",
    "- Split data into 80% train, 20% test using stratified sampling to preserve class balance\n",
    "- Applied standard scaling to linear features for use in linear/neural models\n",
    "- Final shapes show balanced splits with same number of samples but different feature dimensions for tree vs linear\n",
    "- Below i have checked class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check class balance \n",
    "print(\"train target class balance:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest target class balance:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# check for remaining NaNs and impute \n",
    "- doing this here prevents leaked information from the test set into the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "print(\"any NaNs in x_train_linear?\", np.isnan(x_train_linear_scaled).any())\n",
    "print(\"any NaNs in x_test_linear?\", np.isnan(x_test_linear_scaled).any())\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_train_linear_imputed = imputer.fit_transform(x_train_linear)\n",
    "x_test_linear_imputed = imputer.transform(x_test_linear)\n",
    "\n",
    "# now scale \n",
    "scaler = StandardScaler()\n",
    "x_train_linear_scaled = scaler.fit_transform(x_train_linear_imputed)  # Fit on train\n",
    "x_test_linear_scaled = scaler.transform(x_test_linear_imputed) \n",
    "\n",
    "#check shape of imputed data\n",
    "print(\"x_train_linear shape:\", x_train_linear.shape)\n",
    "print(\"x_test_linear shape:\", x_test_linear.shape)\n",
    "#check mean and std of scaled data\n",
    "print(\"mean of x_train_linear_scaled (should be ~0):\", np.mean(x_train_linear_scaled, axis=0)[:5])\n",
    "print(\"std of x_train_linear_scaled (should be ~1):\", np.std(x_train_linear_scaled, axis=0)[:5])\n",
    "#check target shapes \n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# fit models and initial hyperparameter tuning\n",
    "- using randomizedsearchCV, use grid search with a smaller more focused search (computationally expensive)\n",
    "- starting with quick, coarse search (small n_iter, small cv, small sample) as comprehensive search takes too long - ideally use hpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate tree based models \n",
    "\n",
    "# random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# sample 10% of training data \n",
    "sample_frac = 0.1\n",
    "x_train_sample = x_train_tree.sample(frac=sample_frac, random_state=42)\n",
    "y_train_sample = y_train.loc[x_train_sample.index]\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "# fit the model \n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "search_rf = RandomizedSearchCV(\n",
    "    rf, param_distributions=param_dist_rf, n_iter=5, scoring='roc_auc', cv=3, verbose=2, n_jobs=-1\n",
    ") # n_iter = number of random combinations to try, scoring = metric to optimize, cv = number of folds, verbose = print progress, n_jobs = number of cores to use\n",
    "search_rf.fit(x_train_sample, y_train_sample)\n",
    "\n",
    "# refit and evaluate the best model on full training data \n",
    "best_rf_full = RandomForestClassifier(**search_rf.best_params_, random_state=42)\n",
    "best_rf_full.fit(x_train_tree, y_train)\n",
    "y_pred_rf = best_rf_full.predict(x_test_tree)\n",
    "y_proba_rf = best_rf_full.predict_proba(x_test_tree)[:, 1]\n",
    "\n",
    "# evaluate \n",
    "print(\"Random forest classification report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"ROC AUC score:\", roc_auc_score(y_test, y_proba_rf))\n",
    "\n",
    "print(\"Random forest best parameters:\", search_rf.best_params_)\n",
    "print(\"Random forest best score:\", search_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate log reg model \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "param_dist_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# fit model \n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "search_lr = RandomizedSearchCV(\n",
    "    lr, param_distributions=param_dist_lr, n_iter=10, scoring='roc_auc', cv=5, verbose=2, n_jobs=-1\n",
    ")\n",
    "search_lr.fit(x_train_linear_scaled, y_train)\n",
    "\n",
    "best_lr = search_lr.best_estimator_\n",
    "y_pred_lr = best_lr.predict(x_test_linear_scaled)\n",
    "y_proba_lr = best_lr.predict_proba(x_test_linear_scaled)[:, 1]\n",
    "\n",
    "# evaluate \n",
    "print(\"logistic regression classification report\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba_lr))\n",
    "\n",
    "print(\"best parameters:\", search_lr.best_params_)\n",
    "print(\"best score:\", search_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate gradient boosting model \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_gb = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# fit model\n",
    "gb = HistGradientBoostingClassifier()\n",
    "search_gb = RandomizedSearchCV(\n",
    "    gb, param_distributions=param_dist_gb, n_iter=20, scoring='roc_auc', cv=5, verbose=2, n_jobs=-1\n",
    ")\n",
    "search_gb.fit(x_train_tree, y_train)\n",
    "\n",
    "# predict\n",
    "best_gb = search_gb.best_estimator_\n",
    "y_pred_gb = best_gb.predict(x_test_tree)\n",
    "y_proba_gb = best_gb.predict_proba(x_test_tree)[:, 1]\n",
    "\n",
    "# evaluate \n",
    "print(\"grandient boosting classification report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n",
    "print(\"ROC-AUC score:\", roc_auc_score(y_test, y_proba_gb))\n",
    "\n",
    "print(\"best parameters:\", search_gb.best_params_)\n",
    "print(\"best_score:\", search_gb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate XGBoost model \n",
    "import xgboost as xgb \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Number of trees - typical range\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],  # Shallower trees to prevent overfitting\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Smaller learning rates for better generalization\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],  # Sample ratio of training instances\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],  # Sample ratio of features\n",
    "    'min_child_weight': [1, 3, 5, 7],  # Minimum sum of instance weight in child\n",
    "    'gamma': [0, 0.1, 0.2],  # Minimum loss reduction for split\n",
    "    'scale_pos_weight': [1, sum(y_train == 0) / sum(y_train == 1)]  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# fit model \n",
    "xgb_clf = xgb.XGBClassifier(use_labelencoder=False, eval_metric='logloss', random_state=42  )\n",
    "search_xgb = RandomizedSearchCV(\n",
    "    xgb_clf, param_distributions=param_dist_xgb, n_iter=20, scoring='roc_auc', cv=5, verbose=2, n_jobs=-1\n",
    ")\n",
    "search_xgb.fit(x_train_tree, y_train)\n",
    "\n",
    "# predict \n",
    "best_xgb = search_xgb.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(x_test_tree)\n",
    "y_proba_xgb = best_xgb.predict_proba(x_test_tree)[:, 1]\n",
    "\n",
    "# evaluate \n",
    "print(\"XGBoost classification report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"ROC-AUC score:\", roc_auc_score(y_test, y_proba_xgb))\n",
    "\n",
    "print(\"best hyperparameters:\", search_xgb.best_params_)\n",
    "print(\"best score:\", search_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# fit model \n",
    "svc_clf = SVC(kernel='rbf', C=1.0, random_state=42, probability=True)\n",
    "svc_clf.fit(x_train_linear_scaled, y_train)\n",
    "\n",
    "# predict \n",
    "y_pred_svc = svc_clf.predict(x_test_linear_scaled)\n",
    "y_proba_svc = svc_clf.predict_proba(x_test_linear_scaled)[:, 1]\n",
    "\n",
    "# performance\n",
    "print(\"SVC classification report:\")\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"ROC-AUC score:\", roc_auc_score(y_test, y_proba_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (c4_play venv)",
   "language": "python",
   "name": "my_c4_play_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

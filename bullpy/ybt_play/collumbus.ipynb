{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data \n",
    "df = pd.read_csv('/Users/eb2007/Library/CloudStorage/OneDrive-UniversityofCambridge/Documents/PhD/data/YBT.csv')\n",
    "\n",
    "#inspect data \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create target column if not already present \n",
    "if 'autism_diagnosis' not in df.columns:\n",
    "    df['autism_diagnosis'] = (\n",
    "        df['diagnosis']\n",
    "        .fillna('')\n",
    "        .str.lower()\n",
    "        .str.contains('autism')\n",
    "        .astype(int)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values in key columns\n",
    "\n",
    "aq_cols = [f'aq_{i}' for i in range(1, 11)]\n",
    "sq_cols = [f'sq10_{i}' for i in range(1, 11)]\n",
    "eq_cols = [f'eq10_{i}' for i in range(1,11)]\n",
    "demo_cols = ['age', 'sex','gender', 'ethn', 'hand', 'country']\n",
    "\n",
    "#target columns\n",
    "target_col = 'autism_diagnosis' \n",
    "\n",
    "#all colums needed\n",
    "all_cols = aq_cols + sq_cols + eq_cols + demo_cols + [target_col]\n",
    "\n",
    "#check missing values in all columns\n",
    "missing_counts = df[all_cols].isnull().sum()\n",
    "\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting sus column\n",
    "\n",
    "df['diagnosis_69_TEXT'].dropna().head()\n",
    "df['diagnosis_69_TEXT'].notnull().sum()\n",
    "df['diagnosis_69_TEXT'].dropna().unique()\n",
    "df['diagnosis_69_TEXT'].dropna().value_counts()\n",
    "df[['diagnosis_69_TEXT', 'diagnosis','diagnosis_yes_no']].dropna().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint all column names with their question text - wanna know what was before diagnosis_69\n",
    "for col in df.columns:\n",
    "    print(f\"{df.loc[0, col]}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RQ can eq, sq, aq be used to predict diagnosis?\n",
    "\n",
    "#create target column\n",
    "df['autism_diagnosis'] = (\n",
    "    df['diagnosis']\n",
    "    .fillna('') #replace missing values with empty string\n",
    "    .str.lower()\n",
    "    .str.contains('autism')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "print(df['autism_diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle missing values \n",
    "#df_clean = df.dropna(subset=all_cols) - commented out for now\n",
    "#impute missing values instead of dropping added before scaling section\n",
    "\n",
    "\n",
    "#encode categorical variables\n",
    "df_clean = pd.get_dummies(df, columns=['sex', 'gender', 'ethn', 'hand', 'country'], drop_first=True)\n",
    "x = pd.get_dummies(x, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#check distribution of target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_clean['autism_diagnosis'].value_counts().plot(kind='bar')\n",
    "plt.title('Autism Diagnosis Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns that are not needed for ML\n",
    "drop_cols = ['Progress', 'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId', 'diagnosis', 'diagnosis_69_TEXT', 'diagnosis_yes_no', 'English', 'Q573_30_TEXT', 'Q314_30_TEXT', 'Q315_1_TEXT', 'Q318_1_TEXT', 'Q317_1_TEXT', 'Q319_1_TEXT', 'Q311_51_TEXT']\n",
    "df_clean = df_clean.drop(columns=drop_cols, errors='ignore')\n",
    "for col in df_clean.select_dtypes(include=['bool']).columns:\n",
    "    df_clean[col] = df_clean[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = df_clean.drop('autism_diagnosis', axis=1)\n",
    "y = df_clean['autism_diagnosis']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. convert boolean columns to integers\n",
    "for col in x_train.select_dtypes(include=['bool']).columns:\n",
    "    x_train[col] = x_train[col].astype(int)\n",
    "    x_test[col] = x_test[col].astypes(int)\n",
    "\n",
    "# 2. select numeric columns\n",
    "numeric_cols = x_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# 3. impute missing values in numeric columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_train_imputed = pd.DataFrame(imputer.fit_transform(x_train[numeric_cols]), columns=numeric_cols, index=x_train.index)\n",
    "x_test_imputed = pd.DataFrame(imputer.transform(x_test[numeric_cols]), columns=numeric_cols, index=x_test.index)\n",
    "\n",
    "# 4. (optional ting) remove zero-variance columns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "x_train_var_filtered = selector.fit_transform(x_train_imputed)\n",
    "x_test_var_filtered = selector.transform(x_test_imputed)\n",
    "selected_columns = x_train_imputed.columns[selector.get_support()]\n",
    "\n",
    "# 5. scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = pd.DataFrame(scaler.fit_transform(x_train_var_filtered), columns=selected_columns, index=x_train.index)\n",
    "x_test_scaled = pd.DataFrame(scaler.transform(x_test_var_filtered), columns=selected_columns, index=x_test.index)\n",
    "\n",
    "# 6. check for any remaining missing/infinite values\n",
    "print(\"missing values in x_trained_scaled:\", x_train_scaled.isnull().sum().sum())\n",
    "print(\"missing values in x_test_scaled:\", x_test_scaled.isnull().sum().sum())\n",
    "print(\"infinite values in x_train_scaled:\", np.isinf(x_train_scaled.values).sum())\n",
    "print(\"infinite values in x_test_scaled:\", np.isinf(x_test_scaled.values).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training simple LR model with CLEANED data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "#create and train model using CLEANED data\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced') #added class_weight='balanced' to handle class imbalance\n",
    "clf.fit(x_train_scaled, y_train)\n",
    "\n",
    "print(\"model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions \n",
    "y_pred = clf.predict(x_test_scaled)\n",
    "y_pred_proba = clf.predict_proba(x_test_scaled)[:,1] #probability of positive class\n",
    "\n",
    "#evaluate model performance \n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#auc-roc\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"auc-roc: {roc_auc:.4f}\")\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#classification report\n",
    "print(\"\\nclassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking class distribution\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "y_pred_adjusted = (y_pred_proba > threshold).astype(int)\n",
    "print(confusion_matrix(y_test, y_pred_adjusted))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"roc-auc: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load data \n",
    "df = pd.read_csv('/Users/eb2007/Library/CloudStorage/OneDrive-UniversityofCambridge/Documents/PhD/data/YBT.csv')\n",
    "\n",
    "#inspect data \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create target column if not already present \n",
    "if 'autism_diagnosis' not in df.columns:\n",
    "    df['autism_diagnosis'] = (\n",
    "        df['diagnosis']\n",
    "        .fillna('')\n",
    "        .str.lower()\n",
    "        .str.contains('autism')\n",
    "        .astype(int)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values in key columns\n",
    "\n",
    "aq_cols = [f'aq_{i}' for i in range(1, 11)]\n",
    "sq_cols = [f'sq10_{i}' for i in range(1, 11)]\n",
    "eq_cols = [f'eq10_{i}' for i in range(1,11)]\n",
    "demo_cols = ['age', 'sex','gender', 'ethn', 'hand', 'country']\n",
    "\n",
    "#target columns\n",
    "target_col = 'autism_diagnosis' \n",
    "\n",
    "#all colums needed\n",
    "all_cols = aq_cols + sq_cols + eq_cols + demo_cols + [target_col]\n",
    "\n",
    "#check missing values in all columns\n",
    "missing_counts = df[all_cols].isnull().sum()\n",
    "\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting sus column\n",
    "\n",
    "df['diagnosis_69_TEXT'].dropna().head()\n",
    "df['diagnosis_69_TEXT'].notnull().sum()\n",
    "df['diagnosis_69_TEXT'].dropna().unique()\n",
    "df['diagnosis_69_TEXT'].dropna().value_counts()\n",
    "df[['diagnosis_69_TEXT', 'diagnosis','diagnosis_yes_no']].dropna().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint all column names with their question text - wanna know what was before diagnosis_69\n",
    "for col in df.columns:\n",
    "    print(f\"{df.loc[0, col]}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RQ can eq, sq, aq be used to predict diagnosis?\n",
    "\n",
    "#create target column\n",
    "df['autism_diagnosis'] = (\n",
    "    df['diagnosis']\n",
    "    .fillna('') #replace missing values with empty string\n",
    "    .str.lower()\n",
    "    .str.contains('autism')\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "print(df['autism_diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle missing values \n",
    "df_clean = df.dropna(subset=all_cols)\n",
    "\n",
    "#encode categorical variables\n",
    "df_clean = pd.get_dummies(df_clean, columns=['sex', 'gender', 'ethn', 'hand', 'country'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#check distribution of target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_clean['autism_diagnosis'].value_counts().plot(kind='bar')\n",
    "plt.title('Autism Diagnosis Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = df_clean.drop('autism_diagnosis', axis=1)\n",
    "y = df_clean['autism_diagnosis']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress the warnings\n",
    "\n",
    "# Convert boolean columns to numeric and handle object columns\n",
    "bool_columns = x_train.select_dtypes(include=['bool']).columns\n",
    "x_train_numeric = x_train.copy()\n",
    "x_test_numeric = x_test.copy()\n",
    "\n",
    "for col in bool_columns:\n",
    "    x_train_numeric[col] = x_train_numeric[col].astype(int)\n",
    "    x_test_numeric[col] = x_test_numeric[col].astype(int)\n",
    "\n",
    "# Convert object columns that should be numeric\n",
    "object_columns = x_train.select_dtypes(include=['object']).columns\n",
    "for col in object_columns:\n",
    "    try:\n",
    "        x_train_numeric[col] = pd.to_numeric(x_train_numeric[col], errors='coerce')\n",
    "        x_test_numeric[col] = pd.to_numeric(x_test_numeric[col], errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Now select numeric columns\n",
    "numeric_columns = x_train_numeric.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Remove zero-variance columns (optional but recommended)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remove columns with variance < 0.01\n",
    "x_train_var_filtered = selector.fit_transform(x_train_numeric[numeric_columns])\n",
    "x_test_var_filtered = selector.transform(x_test_numeric[numeric_columns])\n",
    "\n",
    "# Get the column names that survived the variance filter\n",
    "selected_columns = numeric_columns[selector.get_support()]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_var_filtered)\n",
    "x_test_scaled = scaler.transform(x_test_var_filtered)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled, columns=selected_columns, index=x_train.index)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, columns=selected_columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug and fix data before training\n",
    "import numpy as np\n",
    "print(\"Checking data before training...\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in x_train_scaled: {x_train_scaled.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y_train: {y_train.isnull().sum()}\")\n",
    "\n",
    "# Clean the data - remove rows with missing or infinite values\n",
    "print(\"Cleaning data...\")\n",
    "mask = ~(x_train_scaled.isnull().any(axis=1) | np.isinf(x_train_scaled.values).any(axis=1))\n",
    "x_train_clean = x_train_scaled[mask]\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "# Also clean the test data\n",
    "mask_test = ~(x_test_scaled.isnull().any(axis=1) | np.isinf(x_test_scaled.values).any(axis=1))\n",
    "x_test_clean = x_test_scaled[mask_test]\n",
    "y_test_clean = y_test[mask_test]\n",
    "\n",
    "print(f\"After cleaning - x_train_clean shape: {x_train_clean.shape}\")\n",
    "print(f\"After cleaning - y_train_clean shape: {y_train_clean.shape}\")\n",
    "print(f\"After cleaning - x_test_clean shape: {x_test_clean.shape}\")\n",
    "print(f\"After cleaning - y_test_clean shape: {y_test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training simple LR model with CLEANED data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#create and train model using CLEANED data\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(x_train_clean, y_train_clean)\n",
    "\n",
    "print(\"model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions \n",
    "y_pred = clf.predict(x_test_clean)\n",
    "y_pred_proba = clf.predict_proba(x_test_clean)[:,1] #probability of positive class\n",
    "\n",
    "#evaluate model performance \n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_clean, y_pred)\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#auc-roc\n",
    "roc_auc = roc_auc_score(y_test_clean, y_pred_proba)\n",
    "print(f\"auc-roc: {roc_auc:.4f}\")\n",
    "\n",
    "#confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_clean, y_pred))\n",
    "\n",
    "#classification report\n",
    "print(\"\\nclassification matrix:\")\n",
    "print(confusion_matrix(y_test_clean, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
